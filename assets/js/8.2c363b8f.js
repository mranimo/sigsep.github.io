(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{277:function(e,t,a){e.exports=a.p+"assets/img/logo_INRIA.f82b8516.svg"},278:function(e,t,a){e.exports=a.p+"assets/img/anr.8a6d9045.jpg"},295:function(e,t,a){"use strict";a.r(t);var r=a(13),s=Object(r.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"introduction"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://joss.theoj.org/papers/571753bc54c5d6dd36382c3d801de41d",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://joss.theoj.org/papers/571753bc54c5d6dd36382c3d801de41d/status.svg",alt:"status"}}),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"}}),t("OutboundLink")],1),e._v(" "),t("a",{attrs:{href:"https://gitter.im/sigsep/open-unmix?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://badges.gitter.im/sigsep/open-unmix.svg",alt:"Gitter"}}),t("OutboundLink")],1)]),e._v(" "),t("h3",{attrs:{id:"open-unmix-a-reference-implementation-for-music-source-separation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#open-unmix-a-reference-implementation-for-music-source-separation"}},[e._v("#")]),e._v(" Open-Unmix - A Reference Implementation for Music Source Separation")]),e._v(" "),t("p",[t("strong",[e._v("Open-Unmix")]),e._v(", is a deep neural network reference implementation for music source separation, applicable for researchers, audio engineers and artists. "),t("strong",[e._v("Open-Unmix")]),e._v(" provides ready-to-use models that allow users to separate pop music into four stems: "),t("strong",[e._v("vocals")]),e._v(", "),t("strong",[e._v("drums")]),e._v(", "),t("strong",[e._v("bass")]),e._v(" and the remaining "),t("strong",[e._v("other")]),e._v(" instruments.")]),e._v(" "),t("p",[e._v("Although "),t("strong",[e._v("open-unmix")]),e._v(" reaches state of the art separation performance as of September, 2019 (See "),t("a",{attrs:{href:"#Evaluation"}},[e._v("Evaluation")]),e._v("), the design choices for it favored simplicity over performance to promote clearness of the code and to have it serve as a "),t("strong",[e._v("baseline")]),e._v(" for future research. The results are comparable/better to those of "),t("code",[e._v("UHL1")]),e._v("/"),t("code",[e._v("UHL2")]),e._v(" which obtained the best performance over all systems trained on MUSDB18 in the "),t("a",{attrs:{href:"https://sisec18.unmix.app",target:"_blank",rel:"noopener noreferrer"}},[e._v("SiSEC 2018 Evaluation campaign"),t("OutboundLink")],1),e._v(".\nWe designed the code to allow researchers to reproduce existing results, quickly develop new architectures and add own user data for training and testing. We favored framework specifics implementations instead of having a monolithic repository with common code for all frameworks.")]),e._v(" "),t("p",[e._v("The model is available for three different frameworks. However, the pytorch implementation serves as the reference version that includes pre-trained networks trained on the "),t("a",{attrs:{href:"https://sigsep.github.io/datasets/musdb.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("MUSDB18"),t("OutboundLink")],1),e._v(" dataset.")]),e._v(" "),t("ul",[t("li",[t("a",{attrs:{href:"https://github.com/sigsep/open-unmix-pytorch",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code Repository"),t("OutboundLink")],1)]),e._v(" "),t("li",[t("a",{attrs:{href:"https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ",target:"_blank",rel:"noopener noreferrer"}},[e._v("Colab notebook"),t("OutboundLink")],1)])]),e._v(" "),t("h2",{attrs:{id:"‚≠êÔ∏è-news"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#‚≠êÔ∏è-news"}},[e._v("#")]),e._v(" ‚≠êÔ∏è News")]),e._v(" "),t("ul",[t("li",[t("p",[e._v("03/07/2021: We added "),t("code",[e._v("umxl")]),e._v(", a model that was trained on extra data which significantly improves the performance, especially generalization.")])]),e._v(" "),t("li",[t("p",[e._v("14/02/2021: We released the new version of open-unmix as a python package. This comes with: a fully differentiable version of "),t("a",{attrs:{href:"https://github.com/sigsep/norbert",target:"_blank",rel:"noopener noreferrer"}},[e._v("norbert"),t("OutboundLink")],1),e._v(", improved audio loading pipeline and large number of bug fixes. See "),t("a",{attrs:{href:"https://github.com/sigsep/open-unmix-pytorch/releases/",target:"_blank",rel:"noopener noreferrer"}},[e._v("release notes"),t("OutboundLink")],1),e._v(" for further info.")])]),e._v(" "),t("li",[t("p",[e._v("06/05/2020: We added a pre-trained speech enhancement model "),t("code",[e._v("umxse")]),e._v(" provided by Sony.")])]),e._v(" "),t("li",[t("p",[e._v("13/03/2020: Open-unmix was awarded 2nd place in the "),t("a",{attrs:{href:"https://devpost.com/software/open-unmix",target:"_blank",rel:"noopener noreferrer"}},[e._v("PyTorch Global Summer Hackathon 2020"),t("OutboundLink")],1),e._v(".")])])]),e._v(" "),t("h2",{attrs:{id:"paper"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#paper"}},[e._v("#")]),e._v(" Paper")]),e._v(" "),t("p",[e._v("Open-unmix is presented in a paper that has been published in the Journal of Open Source Software.\nYou may download the paper PDF "),t("a",{attrs:{href:"https://www.theoj.org/joss-papers/joss.01667/10.21105.joss.01667.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("here"),t("OutboundLink")],1)]),e._v(" "),t("p",[e._v("If you use open-unmix for your research, please cite it through the references "),t("a",{attrs:{href:"#references"}},[e._v("below")]),e._v(".")]),e._v(" "),t("h2",{attrs:{id:"design-choices"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#design-choices"}},[e._v("#")]),e._v(" Design Choices")]),e._v(" "),t("p",[e._v("The design choices made for "),t("em",[e._v("Open-Unmix")]),e._v(" have sought to reach two\nsomewhat contradictory objectives. Its first aim is to have\nstate-of-the-art performance, and its second aim is to still be easily\nunderstandable, so that it can serve as a basis for research to allow\nimproved performance in the future. In the past, many researchers faced\ndifficulties in pre- and post-processing that could be avoided by\nsharing domain knowledge. Our aim was thus to design a system that\nallows researchers to focus on A) new representations and B) new\narchitectures.")]),e._v(" "),t("h3",{attrs:{id:"framework-specific-vs-framework-agnostic"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#framework-specific-vs-framework-agnostic"}},[e._v("#")]),e._v(" Framework specific vs.¬†framework agnostic")]),e._v(" "),t("p",[e._v("We choose "),t("em",[e._v("pytorch")]),e._v(" to serve as a reference implementation due to its\nbalance between simplicity and modularity.\nFurthermore, we already ported the core model to\n"),t("a",{attrs:{href:"https://github.com/sigsep/open-unmix-nnabla",target:"_blank",rel:"noopener noreferrer"}},[e._v("NNabla"),t("OutboundLink")],1),e._v(". Note\nthat the ports will not include pre-trained models as we cannot make\nsure the ports would yield identical results, thus leaving a single\nbaseline model for researchers to compare with.")]),e._v(" "),t("h3",{attrs:{id:"mnist-like"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#mnist-like"}},[e._v("#")]),e._v(' "MNIST-like"')]),e._v(" "),t("p",[e._v("Keeping in mind that the learning curve can be quite steep in audio\nprocessing, we did our best for "),t("em",[e._v("Open-unmix")]),e._v(" to be:")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("simple to extend")]),e._v(": The pre/post-processing, data-loading,\ntraining and models part of the code are isolated and easy to\nreplace/update. In particular, a specific effort was done to make it\neasy to replace the model.")]),e._v(" "),t("li",[t("strong",[e._v("hackable (MNIST like)")]),e._v(": Due to our objective of making it easier\nfor machine-learning experts to try out music separation, we did our\nbest to stick to the philosophy of baseline implementations for this\ncommunity. In particular, "),t("em",[e._v("Open-unmix")]),e._v(" mimics the famous MNIST\nexample, including the ability to instantly start training on a\ndataset that is automatically downloaded.")])]),e._v(" "),t("h3",{attrs:{id:"reproducible"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#reproducible"}},[e._v("#")]),e._v(" Reproducible")]),e._v(" "),t("p",[e._v("Releasing "),t("em",[e._v("Open-Unmix")]),e._v(" is first and foremost an attempt to provide a\nreliable implementation sticking to established programming practice as\nwere also proposed in (McFee et al. 2018). In particular:")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("reproducible code")]),e._v(": everything is provided to exactly reproduce\nour experiments and display our results.")]),e._v(" "),t("li",[t("strong",[e._v("pre-trained models")]),e._v(": we provide pre-trained weights that allow a\nuser to use the model right away or fine-tune it on user-provided\ndata (St√∂ter and Liutkus 2019a, 2019b).")]),e._v(" "),t("li",[t("strong",[e._v("tests")]),e._v(": the release includes unit and regression tests, useful to\norganize future open collaboration through pull requests.")])]),e._v(" "),t("h2",{attrs:{id:"üèÅ-getting-started"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#üèÅ-getting-started"}},[e._v("#")]),e._v(" üèÅ Getting started")]),e._v(" "),t("h3",{attrs:{id:"installation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#installation"}},[e._v("#")]),e._v(" Installation")]),e._v(" "),t("p",[t("code",[e._v("openunmix")]),e._v(" can be installed from pypi using:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v("pip install openunmix\n")])])]),t("p",[e._v("Note, that the pypi version of openunmix uses [torchaudio] to load and save audio files. To increase the number of supported input and output file formats (such as STEMS export), please additionally install "),t("a",{attrs:{href:"https://github.com/faroit/stempeg",target:"_blank",rel:"noopener noreferrer"}},[e._v("stempeg"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v("Training is not part of the open-unmix package, please follow [docs/train.md] for more information.")]),e._v(" "),t("h4",{attrs:{id:"using-docker"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#using-docker"}},[e._v("#")]),e._v(" Using Docker")]),e._v(" "),t("p",[e._v("We also provide a docker container. Performing separation of a local track in "),t("code",[e._v("~/Music/track1.wav")]),e._v(" can be performed in a single line:")]),e._v(" "),t("div",{staticClass:"language- extra-class"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[e._v('docker run -v ~/Music/:/data -it faroit/open-unmix-pytorch umx "/data/track1.wav" --outdir /data/track1\n')])])]),t("h3",{attrs:{id:"pre-trained-models"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#pre-trained-models"}},[e._v("#")]),e._v(" Pre-trained models")]),e._v(" "),t("p",[e._v("We provide three core pre-trained music separation models. All three models are end-to-end models that take waveform inputs and output the separated waveforms.")]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[t("code",[e._v("umxl")])]),e._v("  trained on private stems dataset of compressed stems. "),t("strong",[e._v("Note, that the weights are only licensed for non-commercial use (CC BY-NC-SA 4.0).")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://doi.org/10.5281/zenodo.5069601",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://zenodo.org/badge/DOI/10.5281/zenodo.5069601.svg",alt:"DOI"}}),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[t("strong",[t("code",[e._v("umxhq")]),e._v(" (default)")]),e._v("  trained on "),t("a",{attrs:{href:"https://sigsep.github.io/datasets/musdb.html#uncompressed-wav",target:"_blank",rel:"noopener noreferrer"}},[e._v("MUSDB18-HQ"),t("OutboundLink")],1),e._v(" which comprises the same tracks as in MUSDB18 but un-compressed which yield in a full bandwidth of 22050 Hz.")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://doi.org/10.5281/zenodo.3370489",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://zenodo.org/badge/DOI/10.5281/zenodo.3370489.svg",alt:"DOI"}}),t("OutboundLink")],1)])]),e._v(" "),t("li",[t("p",[t("strong",[t("code",[e._v("umx")])]),e._v(" is trained on the regular "),t("a",{attrs:{href:"https://sigsep.github.io/datasets/musdb.html#compressed-stems",target:"_blank",rel:"noopener noreferrer"}},[e._v("MUSDB18"),t("OutboundLink")],1),e._v(" which is bandwidth limited to 16 kHz do to AAC compression. This model should be used for comparison with other (older) methods for evaluation in "),t("a",{attrs:{href:"sisec18.unmix.app"}},[e._v("SiSEC18")]),e._v(".")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://doi.org/10.5281/zenodo.3370486",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://zenodo.org/badge/DOI/10.5281/zenodo.3370486.svg",alt:"DOI"}}),t("OutboundLink")],1)])])]),e._v(" "),t("p",[e._v("Furthermore, we provide a model for speech enhancement trained by "),t("a",{attrs:{href:"link"}},[e._v("Sony Corporation")])]),e._v(" "),t("ul",[t("li",[t("p",[t("strong",[t("code",[e._v("umxse")])]),e._v(" speech enhancement model is trained on the 28-speaker version of the "),t("a",{attrs:{href:"https://datashare.is.ed.ac.uk/handle/10283/1942?show=full",target:"_blank",rel:"noopener noreferrer"}},[e._v("Voicebank+DEMAND corpus"),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://doi.org/10.5281/zenodo.3786908",target:"_blank",rel:"noopener noreferrer"}},[t("img",{attrs:{src:"https://zenodo.org/badge/DOI/10.5281/zenodo.3786908.svg",alt:"DOI"}}),t("OutboundLink")],1)])])]),e._v(" "),t("p",[e._v("All four models are also available as spectrogram (core) models, which take magnitude spectrogram inputs and ouput separated spectrograms.\nThese models can be loaded using "),t("code",[e._v("umxl_spec")]),e._v(", "),t("code",[e._v("umxhq_spec")]),e._v(", "),t("code",[e._v("umx_spec")]),e._v(" and "),t("code",[e._v("umxse_spec")]),e._v(".")]),e._v(" "),t("p",[e._v("To separate audio files ("),t("code",[e._v("wav")]),e._v(", "),t("code",[e._v("flac")]),e._v(", "),t("code",[e._v("ogg")]),e._v(" - but not "),t("code",[e._v("mp3")]),e._v(") files just run:")]),e._v(" "),t("div",{staticClass:"language-bash extra-class"},[t("pre",{pre:!0,attrs:{class:"language-bash"}},[t("code",[e._v("umx input_file.wav "),t("span",{pre:!0,attrs:{class:"token parameter variable"}},[e._v("--model")]),e._v(" umxl\n")])])]),t("p",[e._v("A more detailed list of the parameters used for the separation is given in the "),t("RouterLink",{attrs:{to:"/docs/inference.html"}},[e._v("inference.md")]),e._v(" document.")],1),e._v(" "),t("p",[e._v("We provide a "),t("a",{attrs:{href:"https://colab.research.google.com/drive/1mijF0zGWxN-KaxTnd0q6hayAlrID5fEQ",target:"_blank",rel:"noopener noreferrer"}},[e._v("jupyter notebook on google colab"),t("OutboundLink")],1),e._v(" to experiment with open-unmix and to separate files online without any installation setup.")]),e._v(" "),t("h3",{attrs:{id:"using-pre-trained-models-from-within-python"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#using-pre-trained-models-from-within-python"}},[e._v("#")]),e._v(" Using pre-trained models from within python")]),e._v(" "),t("p",[e._v("We implementes several ways to load pre-trained models and use them from within your python projects:")]),e._v(" "),t("h4",{attrs:{id:"when-the-package-is-installed"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#when-the-package-is-installed"}},[e._v("#")]),e._v(" When the package is installed")]),e._v(" "),t("p",[e._v("Loading a pre-trained models is as simple as loading")]),e._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[e._v("separator "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" openunmix"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("umxhq"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n")])])]),t("h4",{attrs:{id:"torch-hub"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#torch-hub"}},[e._v("#")]),e._v(" torch.hub")]),e._v(" "),t("p",[e._v("We also provide a torch.hub compatible modules that can be loaded. Note that this does "),t("em",[e._v("not")]),e._v(" even require to install the open-unmix packagen and should generally work when the pytorch version is the same.")]),e._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[e._v("separator "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" torch"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("hub"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("load"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),t("span",{pre:!0,attrs:{class:"token string"}},[e._v("'sigsep/open-unmix-pytorch'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token string"}},[e._v("'umxhq'")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" device"),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v("device"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n")])])]),t("p",[e._v("Where, "),t("code",[e._v("umxhq")]),e._v(" specifies the pre-trained model.")]),e._v(" "),t("h4",{attrs:{id:"performing-separation"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#performing-separation"}},[e._v("#")]),e._v(" Performing separation")]),e._v(" "),t("p",[e._v("With a created separator object, one can perform separation of some "),t("code",[e._v("audio")]),e._v(" (torch.Tensor of shape "),t("code",[e._v("(channels, length)")]),e._v(", provided as at a sampling rate "),t("code",[e._v("separator.sample_rate")]),e._v(") through:")]),e._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[e._v("estimates "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" separator"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("audio"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n"),t("span",{pre:!0,attrs:{class:"token comment"}},[e._v("# returns estimates as tensor")]),e._v("\n")])])]),t("p",[e._v("Note that this requires the audio to be in the right shape and sampling rate. For convenience we provide a pre-processing in "),t("code",[e._v("openunmix.utils.preprocess(..")]),e._v(")` that takes numpy audio and converts it to be used for open-unmix.")]),e._v(" "),t("p",[e._v("To perform model loading, preprocessing and separation in one step, just use:")]),e._v(" "),t("div",{staticClass:"language-python extra-class"},[t("pre",{pre:!0,attrs:{class:"language-python"}},[t("code",[t("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("from")]),e._v(" openunmix "),t("span",{pre:!0,attrs:{class:"token keyword"}},[e._v("import")]),e._v(" separate\nestimates "),t("span",{pre:!0,attrs:{class:"token operator"}},[e._v("=")]),e._v(" separate"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),e._v("predict"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("(")]),e._v("audio"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(",")]),e._v(" "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(".")]),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v(")")]),e._v("\n")])])]),t("h2",{attrs:{id:"contribute-support"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#contribute-support"}},[e._v("#")]),e._v(" Contribute / Support")]),e._v(" "),t("p",[t("em",[e._v("open-unmix")]),e._v(" is a community focused project, we therefore encourage the community to submit bug-fixes and requests for technical support through "),t("a",{attrs:{href:"https://github.com/sigsep/open-unmix-pytorch/issues/new/choose",target:"_blank",rel:"noopener noreferrer"}},[e._v("github issues"),t("OutboundLink")],1),e._v(". For more details of how to contribute, please follow our "),t("a",{attrs:{href:"https://github.com/sigsep/open-unmix-pytorch/blob/master/CONTRIBUTING.md",target:"_blank",rel:"noopener noreferrer"}},[t("code",[e._v("CONTRIBUTING.md")]),t("OutboundLink")],1),e._v(".")]),e._v(" "),t("p",[e._v("For support and help, please use the "),t("a",{attrs:{href:"https://gitter.im/sigsep/open-unmix?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge",target:"_blank",rel:"noopener noreferrer"}},[e._v("gitter chat"),t("OutboundLink")],1),e._v(" or the "),t("a",{attrs:{href:"https://groups.google.com/forum/#!forum/open-unmix",target:"_blank",rel:"noopener noreferrer"}},[e._v("google groups"),t("OutboundLink")],1),e._v(" forum.")]),e._v(" "),t("h2",{attrs:{id:"references"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[e._v("#")]),e._v(" References")]),e._v(" "),t("summary",[e._v("If you use open-unmix for your research ‚Äì Cite Open-Unmix")]),e._v(" "),t("div",{staticClass:"language-latex extra-class"},[t("pre",{pre:!0,attrs:{class:"language-latex"}},[t("code",[e._v("@article"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("stoter19,  \n  author        = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("F.-R. St"),t("span",{pre:!0,attrs:{class:"token function selector"}},[e._v("\\\\")]),e._v('"oter and \n                   S. Uhlich and \n                   A. Liutkus and \n                   Y. Mitsufuji'),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",  \n  title         = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("Open-Unmix - A Reference Implementation \n                   for Music Source Separation"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  journal       = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("Journal of Open Source Software"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",  \n  year          = 2019,\n  doi           = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("10.21105/joss.01667"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  url           = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("https://doi.org/10.21105/joss.01667"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n")])])]),t("p"),e._v(" "),t("summary",[e._v("If you use the MUSDB dataset for your research - Cite the MUSDB18 Dataset")]),e._v(" "),t("p"),t("div",{staticClass:"language-latex extra-class"},[t("pre",{pre:!0,attrs:{class:"language-latex"}},[t("code",[e._v("@misc"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("MUSDB18,\n  author       = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("Zafar Rafii and\n                  Antoine Liutkus and\n                  Fabian-Robert St"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),t("span",{pre:!0,attrs:{class:"token function selector"}},[e._v('\\"')]),e._v("o"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("ter and\n                  Stylianos Ioannis Mimilakis and\n                  Rachel Bittner"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  title        = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("The "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("MUSDB18"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(" corpus for music separation"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  month        = dec,\n  year         = 2017,\n  doi          = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("10.5281/zenodo.1117372"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  url          = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("https://doi.org/10.5281/zenodo.1117372"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n")])])]),t("p"),e._v(" "),t("summary",[e._v("If compare your results with SiSEC 2018 Participants - Cite the SiSEC 2018 LVA/ICA Paper")]),e._v(" "),t("p"),t("div",{staticClass:"language-latex extra-class"},[t("pre",{pre:!0,attrs:{class:"language-latex"}},[t("code",[e._v("@inproceedings"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("SiSEC18,\n  author       = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("Fabian-Robert St"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),t("span",{pre:!0,attrs:{class:"token function selector"}},[e._v('\\"')]),e._v("o"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("ter and\n                  Antoine Liutkus and\n                  Nobutaka Ito"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  title        = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("The 2018 Signal Separation Evaluation Campaign"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  booktitle    = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("Latent Variable Analysis and Signal Separation"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v(",\n  year         = 2018,\n  pages        = "),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("{")]),e._v("293--30"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n"),t("span",{pre:!0,attrs:{class:"token punctuation"}},[e._v("}")]),e._v("\n")])])]),t("p"),e._v(" "),t("p",[e._v("‚ö†Ô∏è Please note that the official acronym for "),t("em",[e._v("open-unmix")]),e._v(" is "),t("strong",[e._v("UMX")]),e._v(".")]),e._v(" "),t("h3",{attrs:{id:"authors"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#authors"}},[e._v("#")]),e._v(" Authors")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://www.faroit.com/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Fabian-Robert St√∂ter"),t("OutboundLink")],1),e._v(", "),t("a",{attrs:{href:"https://github.com/aliutkus",target:"_blank",rel:"noopener noreferrer"}},[e._v("Antoine Liutkus"),t("OutboundLink")],1),e._v(", Inria and LIRMM, Montpellier, France")]),e._v(" "),t("h3",{attrs:{id:"license"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#license"}},[e._v("#")]),e._v(" License")]),e._v(" "),t("p",[e._v("MIT")]),e._v(" "),t("h3",{attrs:{id:"copyright"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#copyright"}},[e._v("#")]),e._v(" Copyright")]),e._v(" "),t("p"),e._v(" "),t("img",{attrs:{src:a(277),width:"250"}}),e._v(" "),t("h3",{attrs:{id:"funding"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#funding"}},[e._v("#")]),e._v(" Funding")]),e._v(" "),t("p",[e._v("This  work  was  partly  supported  by  the  research  programme  KAMoulox(ANR-15-CE38-0003-01)  funded  by  ANR,  the  French  State  agency  for  re-search\n")]),t("p"),e._v(" "),t("img",{attrs:{src:a(278),width:"100"}}),t("p")])}),[],!1,null,null,null);t.default=s.exports}}]);
(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{296:function(e,t,n){"use strict";n.r(t);var a=n(13),i=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"technical-details"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#technical-details"}},[e._v("#")]),e._v(" Technical Details")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vQ1WgVU4PGeEqTQ26j-2RbwaN9ZPlxabBI5N7mYqOK66VjT96UmT9wAaX1s6u6jDHe0ARfAo9E--lQM/pub?w=1918&h=703",alt:""}})]),e._v(" "),t("h3",{attrs:{id:"datasets-and-dataloaders"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#datasets-and-dataloaders"}},[e._v("#")]),e._v(" Datasets and Dataloaders")]),e._v(" "),t("p",[e._v("When designing a machine-learnig based method, our first step is to\nencapsulate cleanly the data-processing aspects.")]),e._v(" "),t("ul",[t("li",[t("strong",[e._v("Datasets")]),e._v(": we support the "),t("em",[e._v("MUSDB18")]),e._v(" which is the most established\ndataset for music separation, that we released some years ago (Rafii\net al. 2017). The dataset contains 150 full-lengths music tracks\n(~10h duration) of different musical styles along with their\nisolated "),t("code",[e._v("drums")]),e._v(", "),t("code",[e._v("bass")]),e._v(", "),t("code",[e._v("vocals")]),e._v(" and "),t("code",[e._v("others")]),e._v(" stems. "),t("em",[e._v("MUSDB18")]),e._v(" is\nsplit into "),t("em",[e._v("training")]),e._v(" (100 songs) and "),t("em",[e._v("test")]),e._v(" subsets (50 songs). All\nfiles from the "),t("em",[e._v("MUSDB18")]),e._v(" dataset are encoded in the Native\nInstruments "),t("a",{attrs:{href:"https://www.native-instruments.com/en/specials/stems/",target:"_blank",rel:"noopener noreferrer"}},[e._v("stems\nformat"),t("OutboundLink")],1),e._v("\n(.mp4) to reduce the file size. It is a multitrack format composed\nof 5 stereo streams, each one encoded in AAC "),t("code",[e._v("@")]),e._v("256kbps. Since AAC\nis bandwidth limited to 16 kHz instead of 22 kHz for full bandwidth,\nany model trained on "),t("em",[e._v("MUSDB18")]),e._v(" would not be able to output\nhigh-quality content. As part of the release of "),t("em",[e._v("Open-Unmix")]),e._v(", we\nalso released "),t("em",[e._v("MUSDB18-HQ")]),e._v(" (Rafii et al. 2019), which is the\nuncompressed, full-quality version of the "),t("em",[e._v("MUSDB18")]),e._v(" dataset.")]),e._v(" "),t("li",[t("strong",[e._v("Efficient data-loading and transforms")]),e._v(": since preparing the\nbatches for training is often the efficiency bottleneck, extra-care\nwas taken to optimize speed and performance. Here, we use a\nframework-specific data loading API instead of a generic module. For\nall frameworks we use the builtin STFT transform operator, when\navailable, that works on the GPU to improve performance (See (Choi,\nJoo, and Kim 2017)).")]),e._v(" "),t("li",[t("strong",[e._v("Essential augmentations")]),e._v(": the data augmentation techniques we\nadopted here for source separation are described in (Uhlich et\nal. 2017). They enable to attain good performance even though the\naudio datasets such as "),t("em",[e._v("MUSDB18")]),e._v(" are often of limited size.")]),e._v(" "),t("li",[t("strong",[e._v("Post processing")]),e._v(": is an important step that helps to improve the\noverall performance by combining the outputs of all instrument DNNs.\nWe use a multichannel Wiener filter (MWF) as was proposed in\n(Nugraha, Liutkus, and Vincent 2016; Sivasankaran et al. 2015) and\nwhich we open-sourced in the\n"),t("a",{attrs:{href:"https://github.com/sigsep/norbert",target:"_blank",rel:"noopener noreferrer"}},[t("code",[e._v("sigsep.norbert")]),t("OutboundLink")],1),e._v(" repository")])]),e._v(" "),t("h3",{attrs:{id:"model"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#model"}},[e._v("#")]),e._v(" Model")]),e._v(" "),t("p",[e._v("The system is trained to predict a separated source from the observation\nof its mixture with other sources. The corresponding training is done in\na "),t("em",[e._v("discriminative")]),e._v(" way, i.e.Â through a dataset of mixtures paired with\ntheir true separated sources. These are used as ground truth targets\nfrom which gradients are computed. Although alternative ways to train a\nseparation system have emerged recently, notably through "),t("em",[e._v("generative")]),e._v("\nstrategies trained through adversarial cost functions, they still did\nnot lead to comparable performance. Even if we acknowledge that such an\napproach could, in theory, allow scaling the size of training data since\nit can be done in an "),t("em",[e._v("unpaired")]),e._v(" manner, we feel that this direction is\nstill in progress and cannot be considered state-of-the-art today. That\nsaid, the "),t("em",[e._v("Open-Unmix")]),e._v(" system can easily be extended to such generative\ntraining, and the community is much welcome to exploit it for that\npurpose.")]),e._v(" "),t("p",[t("img",{attrs:{src:"https://docs.google.com/drawings/d/e/2PACX-1vTPoQiPwmdfET4pZhue1RvG7oEUJz7eUeQvCu6vzYeKRwHl6by4RRTnphImSKM0k5KXw9rZ1iIFnpGW/pub?w=959&h=308",alt:"Separationnetwork\\label{separation_network}"}})]),e._v(" "),t("p",[e._v("The constitutive parts of the actual deep model used in "),t("em",[e._v("Open-Unmix")]),e._v("\nonly comprise very classical elements, depicted in the Figure\nabove.")]),e._v(" "),t("ul",[t("li",[t("em",[e._v("LSTM")]),e._v(": The core of "),t("em",[e._v("Open-Unmix")]),e._v(" is a three-layer bidirectional LSTM\nnetwork (Hochreiter and Schmidhuber 1997). Due to its recurrent\nnature, the model can be trained and evaluated on arbitrary length\nof audio signals. Since the model takes information from the past\nand future simultaneously, the model cannot be used in an\nonline/real-time manner. An uni-directional model can easily be\ntrained.")]),e._v(" "),t("li",[t("em",[e._v("Fully connected time-distributed layers")]),e._v(" are used for\ndimensionality reduction and augmentation, thus encoding/decoding\nthe input and output. They allow control over the number of\nparameters of the model and prove to be crucial for generalization.")]),e._v(" "),t("li",[t("em",[e._v("Skip connections")]),e._v(" are used in two ways: i/ the output to recurrent\nlayers are augmented with their input, and this proved to help\nconvergence. ii/ The output spectrogram is computed as an\nelement-wise multiplication of the input. This means that the system\nhas to learn "),t("em",[e._v("how much each TF bin does belong to the target source")]),e._v("\nand not the "),t("em",[e._v("actual")]),e._v(" value of that bin. This is "),t("em",[e._v("critical")]),e._v(" for\nobtaining good performance and combining the estimates given for\nseveral targets, as done in "),t("em",[e._v("Open-unmix")]),e._v(".")]),e._v(" "),t("li",[t("em",[e._v("Non linearities")]),e._v(" are of three kinds: i/ rectified linear units\n(ReLU) allow intermediate layers to comprise nonnegative\nactivations, which long proved effective in TF modeling. ii/ "),t("code",[e._v("tanh")]),e._v("\nare known to be necessary for good training of LSTM model, notably\nbecause they avoid exploding input and output. iii/ a "),t("code",[e._v("sigmoid")]),e._v("\nactivation is chosen before masking, to mimic the way legacy systems\ntake the outputs as a "),t("em",[e._v("filtering")]),e._v(" of the input.")]),e._v(" "),t("li",[t("em",[e._v("Batch normalization")]),e._v(" long proved important for stable training,\nbecause it makes the different batches more similar in terms of\ndistributions. In the case of audio where signal dynamics can be\nvery important, this is crucial.")])]),e._v(" "),t("p",[e._v("Note that the model can process and predict multichannel spectrograms by\nstacking features. Furthermore, please note that the input and output to\nthe "),t("em",[e._v("Open-Unmix")]),e._v(" core deep model are magnitude spectrograms. Although\nusing phase as additional input feature (Muth et al. 2018) or estimating\nthe instrument phase (Le Roux et al. 2019; Takahashi et al. 2018) are\ninteresting approaches, they have not yet been submitted to\ninternational evaluation campaigns like SiSEC for music separation.")])])}),[],!1,null,null,null);t.default=i.exports}}]);